{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nsns.set()\n\ndf = pd.read_csv('/kaggle/input/oasis-longitudinal/oasis_longitudinal.csv')\ndf.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-12T06:16:45.966355Z","iopub.execute_input":"2023-09-12T06:16:45.966743Z","iopub.status.idle":"2023-09-12T06:16:46.001236Z","shell.execute_reply.started":"2023-09-12T06:16:45.966710Z","shell.execute_reply":"2023-09-12T06:16:46.000080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.loc[df['Visit']==1] # use first visit data only because of the analysis we're doing\ndf = df.reset_index(drop=True) # reset index after filtering first visit data\ndf['M/F'] = df['M/F'].replace(['F','M'], [0,1]) # M/F column\ndf['Group'] = df['Group'].replace(['Converted'], ['Demented']) # Target variable\ndf['Group'] = df['Group'].replace(['Demented', 'Nondemented'], [1,0]) # Target variable\ndf = df.drop(['MRI ID', 'Visit', 'Hand'], axis=1) # Drop unnecessary columns","metadata":{"execution":{"iopub.status.busy":"2023-09-12T06:16:46.003573Z","iopub.execute_input":"2023-09-12T06:16:46.004443Z","iopub.status.idle":"2023-09-12T06:16:46.018526Z","shell.execute_reply.started":"2023-09-12T06:16:46.004392Z","shell.execute_reply":"2023-09-12T06:16:46.017178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bar drawing function\ndef bar_chart(feature):\n    Demented = df[df['Group']==1][feature].value_counts()\n    Nondemented = df[df['Group']==0][feature].value_counts()\n    df_bar = pd.DataFrame([Demented,Nondemented])\n    df_bar.index = ['Demented','Nondemented']\n    df_bar.plot(kind='bar',stacked=True, figsize=(8,5))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T06:16:46.020332Z","iopub.execute_input":"2023-09-12T06:16:46.020989Z","iopub.status.idle":"2023-09-12T06:16:46.030187Z","shell.execute_reply.started":"2023-09-12T06:16:46.020933Z","shell.execute_reply":"2023-09-12T06:16:46.028984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gender  and  Group ( Femal=0, Male=1)\nbar_chart('M/F')\nplt.xlabel('Group')\nplt.ylabel('Number of patients')\nplt.legend()\nplt.title('Gender and Demented rate')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T06:16:46.032644Z","iopub.execute_input":"2023-09-12T06:16:46.033822Z","iopub.status.idle":"2023-09-12T06:16:46.512760Z","shell.execute_reply.started":"2023-09-12T06:16:46.033771Z","shell.execute_reply":"2023-09-12T06:16:46.511273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MMSE : Mini Mental State Examination\n# Nondemented = 0, Demented =1\n# Nondemented has higher test result ranging from 25 to 30. \n#Min 17 ,MAX 30\nfacet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\nfacet.map(sns.kdeplot,'MMSE',shade= True)\nfacet.set(xlim=(0, df['MMSE'].max()))\nfacet.add_legend()\nplt.xlim(15.30)     ","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:02:14.127847Z","iopub.execute_input":"2023-09-12T07:02:14.128315Z","iopub.status.idle":"2023-09-12T07:02:15.061028Z","shell.execute_reply.started":"2023-09-12T07:02:14.128280Z","shell.execute_reply":"2023-09-12T07:02:15.059780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bar_chart('ASF') = Atlas Scaling Factor\nfacet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\nfacet.map(sns.kdeplot,'ASF',shade= True)\nfacet.set(xlim=(0, df['ASF'].max()))\nfacet.add_legend()\nplt.xlim(0.5, 2)\n\n#eTIV = Estimated Total Intracranial Volume\nfacet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\nfacet.map(sns.kdeplot,'eTIV',shade= True)\nfacet.set(xlim=(0, df['eTIV'].max()))\nfacet.add_legend()\nplt.xlim(900, 2100)\n\n#'nWBV' = Normalized Whole Brain Volume\n# Nondemented = 0, Demented =1\nfacet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\nfacet.map(sns.kdeplot,'nWBV',shade= True)\nfacet.set(xlim=(0, df['nWBV'].max()))\nfacet.add_legend()\nplt.xlim(0.6,0.9)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:02:43.336590Z","iopub.execute_input":"2023-09-12T07:02:43.337450Z","iopub.status.idle":"2023-09-12T07:02:46.001616Z","shell.execute_reply.started":"2023-09-12T07:02:43.337409Z","shell.execute_reply":"2023-09-12T07:02:46.000401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#AGE. Nondemented =0, Demented =0\nfacet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, df['Age'].max()))\nfacet.add_legend()\nplt.xlim(50,100)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:03:03.452465Z","iopub.execute_input":"2023-09-12T07:03:03.452873Z","iopub.status.idle":"2023-09-12T07:03:04.356103Z","shell.execute_reply.started":"2023-09-12T07:03:03.452825Z","shell.execute_reply":"2023-09-12T07:03:04.354900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#'EDUC' = Years of Education\n# Nondemented = 0, Demented =1\nfacet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\nfacet.map(sns.kdeplot,'EDUC',shade= True)\nfacet.set(xlim=(df['EDUC'].min(), df['EDUC'].max()))\nfacet.add_legend()\nplt.ylim(0, 0.16)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:03:18.562957Z","iopub.execute_input":"2023-09-12T07:03:18.563346Z","iopub.status.idle":"2023-09-12T07:03:19.396907Z","shell.execute_reply.started":"2023-09-12T07:03:18.563316Z","shell.execute_reply":"2023-09-12T07:03:19.394845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check missing values by each column\npd.isnull(df).sum() \n# The column, SES has 8 missing values","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:03:33.402961Z","iopub.execute_input":"2023-09-12T07:03:33.403349Z","iopub.status.idle":"2023-09-12T07:03:33.412326Z","shell.execute_reply.started":"2023-09-12T07:03:33.403319Z","shell.execute_reply":"2023-09-12T07:03:33.411096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropped the 8 rows with missing values in the column, SES\ndf_dropna = df.dropna(axis=0, how='any')\npd.isnull(df_dropna).sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:03:47.342596Z","iopub.execute_input":"2023-09-12T07:03:47.342992Z","iopub.status.idle":"2023-09-12T07:03:47.354325Z","shell.execute_reply.started":"2023-09-12T07:03:47.342960Z","shell.execute_reply":"2023-09-12T07:03:47.353172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_dropna['Group'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:03:59.861908Z","iopub.execute_input":"2023-09-12T07:03:59.862309Z","iopub.status.idle":"2023-09-12T07:03:59.871294Z","shell.execute_reply.started":"2023-09-12T07:03:59.862276Z","shell.execute_reply":"2023-09-12T07:03:59.870384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw scatter plot between EDUC and SES\nx = df['EDUC']\ny = df['SES']\n\nses_not_null_index = y[~y.isnull()].index\nx = x[ses_not_null_index]\ny = y[ses_not_null_index]\n\n# Draw trend line in red\nz = np.polyfit(x, y, 1)\np = np.poly1d(z)\nplt.plot(x, y, 'go', x, p(x), \"r--\")\nplt.xlabel('Education Level(EDUC)')\nplt.ylabel('Social Economic Status(SES)')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:04:16.415076Z","iopub.execute_input":"2023-09-12T07:04:16.415678Z","iopub.status.idle":"2023-09-12T07:04:16.734901Z","shell.execute_reply.started":"2023-09-12T07:04:16.415643Z","shell.execute_reply":"2023-09-12T07:04:16.733546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(['EDUC'])['SES'].median()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:04:37.273090Z","iopub.execute_input":"2023-09-12T07:04:37.274277Z","iopub.status.idle":"2023-09-12T07:04:37.288761Z","shell.execute_reply.started":"2023-09-12T07:04:37.274227Z","shell.execute_reply":"2023-09-12T07:04:37.287314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"SES\"].fillna(df.groupby(\"EDUC\")[\"SES\"].transform(\"median\"), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:05:06.287685Z","iopub.execute_input":"2023-09-12T07:05:06.288438Z","iopub.status.idle":"2023-09-12T07:05:06.294530Z","shell.execute_reply.started":"2023-09-12T07:05:06.288398Z","shell.execute_reply":"2023-09-12T07:05:06.293661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I confirm there're no more missing values and all the 150 data were used.\npd.isnull(df['SES']).value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:05:22.935047Z","iopub.execute_input":"2023-09-12T07:05:22.936150Z","iopub.status.idle":"2023-09-12T07:05:22.948230Z","shell.execute_reply.started":"2023-09-12T07:05:22.936106Z","shell.execute_reply":"2023-09-12T07:05:22.947151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler \nfrom sklearn.model_selection import cross_val_score","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:05:36.917387Z","iopub.execute_input":"2023-09-12T07:05:36.917993Z","iopub.status.idle":"2023-09-12T07:05:37.126086Z","shell.execute_reply.started":"2023-09-12T07:05:36.917959Z","shell.execute_reply":"2023-09-12T07:05:37.124909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset with imputation\nY = df['Group'].values # Target for the model\nX = df[['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']] # Features we use\n\n# splitting into three sets\nX_trainval, X_test, Y_trainval, Y_test = train_test_split(\n    X, Y, random_state=0)\n\n# Feature scaling\nscaler = MinMaxScaler().fit(X_trainval)\nX_trainval_scaled = scaler.transform(X_trainval)\nX_test_scaled = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:05:51.614489Z","iopub.execute_input":"2023-09-12T07:05:51.614933Z","iopub.status.idle":"2023-09-12T07:05:51.633830Z","shell.execute_reply.started":"2023-09-12T07:05:51.614886Z","shell.execute_reply":"2023-09-12T07:05:51.632365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset after dropping missing value rows\nY = df_dropna['Group'].values # Target for the model\nX = df_dropna[['M/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF']] # Features we use\n\n# splitting into three sets\nX_trainval_dna, X_test_dna, Y_trainval_dna, Y_test_dna = train_test_split(\n    X, Y, random_state=0)\n\n# Feature scaling\nscaler = MinMaxScaler().fit(X_trainval_dna)\nX_trainval_scaled_dna = scaler.transform(X_trainval_dna)\nX_test_scaled_dna = scaler.transform(X_test_dna)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:06:18.324729Z","iopub.execute_input":"2023-09-12T07:06:18.325198Z","iopub.status.idle":"2023-09-12T07:06:18.340669Z","shell.execute_reply.started":"2023-09-12T07:06:18.325158Z","shell.execute_reply":"2023-09-12T07:06:18.339535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, roc_curve, auc","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:07:29.925696Z","iopub.execute_input":"2023-09-12T07:07:29.926123Z","iopub.status.idle":"2023-09-12T07:07:30.357674Z","shell.execute_reply.started":"2023-09-12T07:07:29.926087Z","shell.execute_reply":"2023-09-12T07:07:30.356501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = [] # list to store all performance metric","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:08:12.062309Z","iopub.execute_input":"2023-09-12T07:08:12.063293Z","iopub.status.idle":"2023-09-12T07:08:12.067706Z","shell.execute_reply.started":"2023-09-12T07:08:12.063251Z","shell.execute_reply":"2023-09-12T07:08:12.066489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset with imputation\nbest_score=0\nkfolds=5 # set the number of folds\n\nfor c in [0.001, 0.1, 1, 10, 100]:\n    logRegModel = LogisticRegression(C=c)\n    # perform cross-validation\n    scores = cross_val_score(logRegModel, X_trainval, Y_trainval, cv=kfolds, scoring='accuracy') # Get recall for each parameter setting\n    \n    # compute mean cross-validation accuracy\n    score = np.mean(scores)\n    \n    # Find the best parameters and score\n    if score > best_score:\n        best_score = score\n        best_parameters = c\n\n# rebuild a model on the combined training and validation set\nSelectedLogRegModel = LogisticRegression(C=best_parameters).fit(X_trainval_scaled, Y_trainval)\n\ntest_score = SelectedLogRegModel.score(X_test_scaled, Y_test)\nPredictedOutput = SelectedLogRegModel.predict(X_test_scaled)\ntest_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\nfpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\ntest_auc = auc(fpr, tpr)\nprint(\"Best accuracy on validation set is:\", best_score)\nprint(\"Best parameter for regularization (C) is: \", best_parameters)\nprint(\"Test accuracy with best C parameter is\", test_score)\nprint(\"Test recall with the best C parameter is\", test_recall)\nprint(\"Test AUC with the best C parameter is\", test_auc)\nm = 'Logistic Regression (w/ imputation)'\nacc.append([m, test_score, test_recall, test_auc, fpr, tpr, thresholds])\n","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:09:31.776650Z","iopub.execute_input":"2023-09-12T07:09:31.777112Z","iopub.status.idle":"2023-09-12T07:09:32.407874Z","shell.execute_reply.started":"2023-09-12T07:09:31.777075Z","shell.execute_reply":"2023-09-12T07:09:32.406570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset after dropping missing value rows\nbest_score=0\nkfolds=5 # set the number of folds\n\nfor c in [0.001, 0.1, 1, 10, 100]:\n    logRegModel = LogisticRegression(C=c)\n    # perform cross-validation\n    scores = cross_val_score(logRegModel, X_trainval_scaled_dna, Y_trainval_dna, cv=kfolds, scoring='accuracy')\n    \n    # compute mean cross-validation accuracy\n    score = np.mean(scores)\n    \n    # Find the best parameters and score\n    if score > best_score:\n        best_score = score\n        best_parameters = c\n\n# rebuild a model on the combined training and validation set\nSelectedLogRegModel = LogisticRegression(C=best_parameters).fit(X_trainval_scaled_dna, Y_trainval_dna)\n\ntest_score = SelectedLogRegModel.score(X_test_scaled_dna, Y_test_dna)\nPredictedOutput = SelectedLogRegModel.predict(X_test_scaled)\ntest_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\nfpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\ntest_auc = auc(fpr, tpr)\nprint(\"Best accuracy on validation set is:\", best_score)\nprint(\"Best parameter for regularization (C) is: \", best_parameters)\nprint(\"Test accuracy with best C parameter is\", test_score)        \nprint(\"Test recall with the best C parameter is\", test_recall)\nprint(\"Test AUC with the best C parameter is\", test_auc)\n\nm = 'Logistic Regression (w/ dropna)'\nacc.append([m, test_score, test_recall, test_recall, fpr, tpr, thresholds])","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:10:26.106230Z","iopub.execute_input":"2023-09-12T07:10:26.106623Z","iopub.status.idle":"2023-09-12T07:10:26.301985Z","shell.execute_reply.started":"2023-09-12T07:10:26.106592Z","shell.execute_reply":"2023-09-12T07:10:26.300939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_score = 0\n\nfor c_paramter in [0.001, 0.01, 0.1, 1, 10, 100, 1000]: #iterate over the values we need to try for the parameter C\n    for gamma_paramter in [0.001, 0.01, 0.1, 1, 10, 100, 1000]: #iterate over the values we need to try for the parameter gamma\n        for k_parameter in ['rbf', 'linear', 'poly', 'sigmoid']: # iterate over the values we need to try for the kernel parameter\n            svmModel = SVC(kernel=k_parameter, C=c_paramter, gamma=gamma_paramter) #define the model\n            # perform cross-validation\n            scores = cross_val_score(svmModel, X_trainval_scaled, Y_trainval, cv=kfolds, scoring='accuracy')\n            # the training set will be split internally into training and cross validation\n\n            # compute mean cross-validation accuracy\n            score = np.mean(scores)\n            # if we got a better score, store the score and parameters\n            if score > best_score:\n                best_score = score #store the score \n                best_parameter_c = c_paramter #store the parameter c\n                best_parameter_gamma = gamma_paramter #store the parameter gamma\n                best_parameter_k = k_parameter\n            \n\n# rebuild a model with best parameters to get score \nSelectedSVMmodel = SVC(C=best_parameter_c, gamma=best_parameter_gamma, kernel=best_parameter_k).fit(X_trainval_scaled, Y_trainval)\n\ntest_score = SelectedSVMmodel.score(X_test_scaled, Y_test)\nPredictedOutput = SelectedSVMmodel.predict(X_test_scaled)\ntest_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\nfpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\ntest_auc = auc(fpr, tpr)\nprint(\"Best accuracy on cross validation set is:\", best_score)\nprint(\"Best parameter for c is: \", best_parameter_c)\nprint(\"Best parameter for gamma is: \", best_parameter_gamma)\nprint(\"Best parameter for kernel is: \", best_parameter_k)\nprint(\"Test accuracy with the best parameters is\", test_score)\nprint(\"Test recall with the best parameters is\", test_recall)\nprint(\"Test recall with the best parameter is\", test_auc)\n\nm = 'SVM'\nacc.append([m, test_score, test_recall, test_auc, fpr, tpr, thresholds])","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:11:37.679442Z","iopub.execute_input":"2023-09-12T07:11:37.679913Z","iopub.status.idle":"2023-09-12T07:11:44.596815Z","shell.execute_reply.started":"2023-09-12T07:11:37.679874Z","shell.execute_reply":"2023-09-12T07:11:44.595643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_score = 0\n\nfor md in range(1, 9): # iterate different maximum depth values\n    # train the model\n    treeModel = DecisionTreeClassifier(random_state=0, max_depth=md, criterion='gini')\n    # perform cross-validation\n    scores = cross_val_score(treeModel, X_trainval_scaled, Y_trainval, cv=kfolds, scoring='accuracy')\n    \n    # compute mean cross-validation accuracy\n    score = np.mean(scores)\n    \n    # if we got a better score, store the score and parameters\n    if score > best_score:\n        best_score = score\n        best_parameter = md\n\n# Rebuild a model on the combined training and validation set        \nSelectedDTModel = DecisionTreeClassifier(max_depth=best_parameter).fit(X_trainval_scaled, Y_trainval )\n\ntest_score = SelectedDTModel.score(X_test_scaled, Y_test)\nPredictedOutput = SelectedDTModel.predict(X_test_scaled)\ntest_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\nfpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\ntest_auc = auc(fpr, tpr)\nprint(\"Best accuracy on validation set is:\", best_score)\nprint(\"Best parameter for the maximum depth is: \", best_parameter)\nprint(\"Test accuracy with best parameter is \", test_score)\nprint(\"Test recall with best parameters is \", test_recall)\nprint(\"Test AUC with the best parameter is \", test_auc)\n\nm = 'Decision Tree'\nacc.append([m, test_score, test_recall, test_auc, fpr, tpr, thresholds])","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:12:57.403811Z","iopub.execute_input":"2023-09-12T07:12:57.404959Z","iopub.status.idle":"2023-09-12T07:12:57.521203Z","shell.execute_reply.started":"2023-09-12T07:12:57.404906Z","shell.execute_reply":"2023-09-12T07:12:57.519897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Feature importance: \")\nnp.array([X.columns.values.tolist(), list(SelectedDTModel.feature_importances_)]).T","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:13:20.780197Z","iopub.execute_input":"2023-09-12T07:13:20.781232Z","iopub.status.idle":"2023-09-12T07:13:20.791285Z","shell.execute_reply.started":"2023-09-12T07:13:20.781184Z","shell.execute_reply":"2023-09-12T07:13:20.790009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import export_graphviz\nimport graphviz \ndot_data=export_graphviz(SelectedDTModel, feature_names=X_trainval.columns.values.tolist(),out_file=None)\ngraph = graphviz.Source(dot_data)  \ngraph ","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:13:52.341905Z","iopub.execute_input":"2023-09-12T07:13:52.343074Z","iopub.status.idle":"2023-09-12T07:13:52.624921Z","shell.execute_reply.started":"2023-09-12T07:13:52.343033Z","shell.execute_reply":"2023-09-12T07:13:52.623581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_score = 0\n\nfor M in range(2, 15, 2): # combines M trees\n    for d in range(1, 9): # maximum number of features considered at each split\n        for m in range(1, 9): # maximum depth of the tree\n            # train the model\n            # n_jobs(4) is the number of parallel computing\n            forestModel = RandomForestClassifier(n_estimators=M, max_features=d, n_jobs=4,\n                                          max_depth=m, random_state=0)\n        \n            # perform cross-validation\n            scores = cross_val_score(forestModel, X_trainval_scaled, Y_trainval, cv=kfolds, scoring='accuracy')\n\n            # compute mean cross-validation accuracy\n            score = np.mean(scores)\n\n            # if we got a better score, store the score and parameters\n            if score > best_score:\n                best_score = score\n                best_M = M\n                best_d = d\n                best_m = m\n\n# Rebuild a model on the combined training and validation set        \nSelectedRFModel = RandomForestClassifier(n_estimators=M, max_features=d,\n                                          max_depth=m, random_state=0).fit(X_trainval_scaled, Y_trainval )\n\nPredictedOutput = SelectedRFModel.predict(X_test_scaled)\ntest_score = SelectedRFModel.score(X_test_scaled, Y_test)\ntest_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\nfpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\ntest_auc = auc(fpr, tpr)\nprint(\"Best accuracy on validation set is:\", best_score)\nprint(\"Best parameters of M, d, m are: \", best_M, best_d, best_m)\nprint(\"Test accuracy with the best parameters is\", test_score)\nprint(\"Test recall with the best parameters is:\", test_recall)\nprint(\"Test AUC with the best parameters is:\", test_auc)\n\nm = 'Random Forest'\nacc.append([m, test_score, test_recall, test_auc, fpr, tpr, thresholds])","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:16:27.224457Z","iopub.execute_input":"2023-09-12T07:16:27.225044Z","iopub.status.idle":"2023-09-12T07:18:15.399629Z","shell.execute_reply.started":"2023-09-12T07:16:27.224999Z","shell.execute_reply":"2023-09-12T07:18:15.398143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Feature importance: \")\nnp.array([X.columns.values.tolist(), list(SelectedRFModel.feature_importances_)]).T","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:18:51.265993Z","iopub.execute_input":"2023-09-12T07:18:51.266427Z","iopub.status.idle":"2023-09-12T07:18:51.278150Z","shell.execute_reply.started":"2023-09-12T07:18:51.266391Z","shell.execute_reply":"2023-09-12T07:18:51.276931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_score = 0\n\nfor M in range(2, 15, 2): # combines M trees\n    for lr in [0.0001, 0.001, 0.01, 0.1, 1]:\n        # train the model\n        boostModel = AdaBoostClassifier(n_estimators=M, learning_rate=lr, random_state=0)\n\n        # perform cross-validation\n        scores = cross_val_score(boostModel, X_trainval_scaled, Y_trainval, cv=kfolds, scoring='accuracy')\n\n        # compute mean cross-validation accuracy\n        score = np.mean(scores)\n\n        # if we got a better score, store the score and parameters\n        if score > best_score:\n            best_score = score\n            best_M = M\n            best_lr = lr\n\n# Rebuild a model on the combined training and validation set        \nSelectedBoostModel = AdaBoostClassifier(n_estimators=M, learning_rate=lr, random_state=0).fit(X_trainval_scaled, Y_trainval )\n\nPredictedOutput = SelectedBoostModel.predict(X_test_scaled)\ntest_score = SelectedRFModel.score(X_test_scaled, Y_test)\ntest_recall = recall_score(Y_test, PredictedOutput, pos_label=1)\nfpr, tpr, thresholds = roc_curve(Y_test, PredictedOutput, pos_label=1)\ntest_auc = auc(fpr, tpr)\nprint(\"Best accuracy on validation set is:\", best_score)\nprint(\"Best parameter of M is: \", best_M)\nprint(\"best parameter of LR is: \", best_lr)\nprint(\"Test accuracy with the best parameter is\", test_score)\nprint(\"Test recall with the best parameters is:\", test_recall)\nprint(\"Test AUC with the best parameters is:\", test_auc)\n\nm = 'AdaBoost'\nacc.append([m, test_score, test_recall, test_auc, fpr, tpr, thresholds])","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:19:34.745352Z","iopub.execute_input":"2023-09-12T07:19:34.745798Z","iopub.status.idle":"2023-09-12T07:19:38.261116Z","shell.execute_reply.started":"2023-09-12T07:19:34.745761Z","shell.execute_reply":"2023-09-12T07:19:38.259952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Feature importance: \")\nnp.array([X.columns.values.tolist(), list(SelectedBoostModel.feature_importances_)]).T","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:20:26.478167Z","iopub.execute_input":"2023-09-12T07:20:26.478578Z","iopub.status.idle":"2023-09-12T07:20:26.490456Z","shell.execute_reply.started":"2023-09-12T07:20:26.478545Z","shell.execute_reply":"2023-09-12T07:20:26.488962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Performance Metric for each model\nresult = pd.DataFrame(acc, columns=['Model', 'Accuracy', 'Recall', 'AUC', 'FPR', 'TPR', 'TH'])\nresult[['Model', 'Accuracy', 'Recall', 'AUC']]","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:21:38.882210Z","iopub.execute_input":"2023-09-12T07:21:38.882958Z","iopub.status.idle":"2023-09-12T07:21:38.900294Z","shell.execute_reply.started":"2023-09-12T07:21:38.882922Z","shell.execute_reply":"2023-09-12T07:21:38.899051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}